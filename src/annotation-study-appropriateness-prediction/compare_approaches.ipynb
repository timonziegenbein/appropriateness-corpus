{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "534a1bda-e8fd-4dc8-92e7-8c858aa065de",
   "metadata": {},
   "source": [
    "### This file compares the approaches for predicting appropriateness (table 4 in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2316415-dc72-49d7-9bbe-93f641c160b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import wilcoxon\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648be460-2b0c-46c3-9703-ca1a9d5adc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTDIMS = [\n",
    "    'eval_Inappropriateness_macroF1',\n",
    "    'eval_Inappropriateness_precision',\n",
    "    'eval_Inappropriateness_recall',\n",
    "    'eval_Missing Intelligibility_macroF1',\n",
    "    'eval_Missing Intelligibility_precision',\n",
    "    'eval_Missing Intelligibility_recall',\n",
    "    'eval_Confusing Reasoning_macroF1',\n",
    "    'eval_Confusing Reasoning_precision',\n",
    "    'eval_Confusing Reasoning_recall',\n",
    "    'eval_Unclear Meaning_macroF1',\n",
    "    'eval_Unclear Meaning_precision',\n",
    "    'eval_Unclear Meaning_recall',\n",
    "    'eval_Missing Relevance_macroF1',\n",
    "    'eval_Missing Relevance_precision',\n",
    "    'eval_Missing Relevance_recall',\n",
    "    'eval_Missing Commitment_macroF1',\n",
    "    'eval_Missing Commitment_precision',\n",
    "    'eval_Missing Commitment_recall',\n",
    "    'eval_Missing Openness_macroF1',\n",
    "    'eval_Missing Openness_precision',\n",
    "    'eval_Missing Openness_recall',\n",
    "    'eval_Missing Seriousness_macroF1',\n",
    "    'eval_Missing Seriousness_precision',\n",
    "    'eval_Missing Seriousness_recall',\n",
    "    'eval_Excessive Intensity_macroF1',\n",
    "    'eval_Excessive Intensity_precision',\n",
    "    'eval_Excessive Intensity_recall',\n",
    "    'eval_Emotional Deception_macroF1',\n",
    "    'eval_Emotional Deception_precision',\n",
    "    'eval_Emotional Deception_recall',\n",
    "    'eval_Toxic Emotions_macroF1',\n",
    "    'eval_Toxic Emotions_precision',\n",
    "    'eval_Toxic Emotions_recall',\n",
    "    'eval_Reason Unclassified_macroF1',\n",
    "    'eval_Reason Unclassified_precision',\n",
    "    'eval_Reason Unclassified_recall',\n",
    "    'eval_Detrimental Orthography_macroF1',\n",
    "    'eval_Detrimental Orthography_precision',\n",
    "    'eval_Detrimental Orthography_recall',\n",
    "    'eval_Other Reasons_macroF1',\n",
    "    'eval_Other Reasons_precision',\n",
    "    'eval_Other Reasons_recall',\n",
    "    'eval_mean_F1',\n",
    "    'eval_mean_precision',\n",
    "    'eval_mean_recall'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791997c-1ee5-4dba-abce-7187f7263afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '../../data/models/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271a8a3-8e8a-498f-be7f-abf141a4b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "approaches = [\n",
    "    'human-upperbound',\n",
    "    'majority-baseline',\n",
    "    'multilabel-roberta-baseline',\n",
    "    'multilabel-roberta-baseline-shuffle',\n",
    "    'multilabel-roberta-baseline-wo-issue',\n",
    "    'random-baseline'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de7adc-6efb-4eaa-897c-9489556e5ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average all the folds\n",
    "test_dict = {x: [] for x in RESULTDIMS}\n",
    "for approach in approaches:\n",
    "    tmp_results = []\n",
    "    for repeat in range(5):\n",
    "        for k in range(5):\n",
    "            with open(model_dir+approach+'/fold{}.{}/test_results.json'.format(repeat,k), 'r') as f:\n",
    "                tmp_result = json.load(f)\n",
    "            tmp_results.append(tmp_result)\n",
    "    d = {}\n",
    "    for k, _ in tmp_results[0].items():\n",
    "        d[k] = np.std([d[k] for d in tmp_results])\n",
    "    for dim in RESULTDIMS:\n",
    "        test_dict[dim].append(d[dim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b22e7a-2dcc-4e8d-9051-2c09eddd0d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict['approach'] = approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82b8f63-40e3-4a47-bd27-c34a3c66ac8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=test_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f5cbe0-a07d-4518-81a7-53fd9197e0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print F1-scores (table 4 in the paper)\n",
    "df[[\n",
    "    'approach',\n",
    "    'eval_mean_F1',\n",
    "    'eval_Inappropriateness_macroF1',\n",
    "    'eval_Toxic Emotions_macroF1',\n",
    "    'eval_Excessive Intensity_macroF1',\n",
    "    'eval_Emotional Deception_macroF1',\n",
    "    'eval_Missing Commitment_macroF1',\n",
    "    'eval_Missing Seriousness_macroF1',\n",
    "    'eval_Missing Openness_macroF1',\n",
    "    'eval_Missing Intelligibility_macroF1',\n",
    "    'eval_Unclear Meaning_macroF1',\n",
    "    'eval_Missing Relevance_macroF1',\n",
    "    'eval_Confusing Reasoning_macroF1',\n",
    "    'eval_Other Reasons_macroF1',\n",
    "    'eval_Detrimental Orthography_macroF1',\n",
    "    'eval_Reason Unclassified_macroF1',\n",
    "]].sort_values('eval_mean_F1', ascending=False).round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a6a0f0-a752-457d-b7bb-8cf886e61246",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {x: [] for x in RESULTDIMS}\n",
    "for approach in approaches:\n",
    "    tmp_results = []\n",
    "    for repeat in range(5):\n",
    "        for k in range(5):\n",
    "            with open(model_dir+approach+'/fold{}.{}/test_results.json'.format(repeat,k), 'r') as f:\n",
    "                tmp_result = json.load(f)\n",
    "            tmp_results.append(tmp_result)\n",
    "    for dim in RESULTDIMS:\n",
    "        test_dict[dim].append([x[dim] for x in tmp_results])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a9918-d3e9-42b9-be23-a158098bdea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check significance of all approaches\n",
    "for dim in RESULTDIMS:\n",
    "    if 'F1' in dim:\n",
    "        for i, approach1 in enumerate(approaches):\n",
    "            for j, approach2 in enumerate(approaches):\n",
    "                if i<j:\n",
    "                    w, p = wilcoxon(test_dict[dim][i], test_dict[dim][j], mode='exact')\n",
    "                    print((dim, approach1, approach2))\n",
    "                    print(p<=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f865fe0-acc1-497c-9077-209b550bd2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
