{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "656ea829-530d-4346-86ee-fd6fc87f0b6c",
   "metadata": {},
   "source": [
    "### This file creates the trivial baselines for the paper (table 4 in the paper) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0eacc9-846e-4607-a760-fb8626d9086b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import json\n",
    "import wandb\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from pathlib import Path\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1e6888-c253-49b2-8ee3-eea8dc6a5f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIMS = [\n",
    "    'Inappropriateness',\n",
    "    'Toxic Emotions',\n",
    "    'Excessive Intensity',\n",
    "    'Emotional Deception',\n",
    "    'Missing Commitment',\n",
    "    'Missing Seriousness',\n",
    "    'Missing Openness',\n",
    "    'Missing Intelligibility',\n",
    "    'Unclear Meaning',\n",
    "    'Missing Relevance',\n",
    "    'Confusing Reasoning',\n",
    "    'Other Reasons',\n",
    "    'Detrimental Orthography',\n",
    "    'Reason Unclassified'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d0426-96d5-40f3-a7dc-43f57b988f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de031de-e416-4ec1-b5cb-7da7342a7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_conservative_w_folds.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce571277-7ccd-43e9-affe-9ffa6f4d096e",
   "metadata": {},
   "source": [
    "#### Human-upperbound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7d94bb-bcf6-43db-ba97-3971771cb854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for repeat in range(5):\n",
    "    for k in range(5):\n",
    "        out_dicts = []\n",
    "        for i in [6,7,8]:\n",
    "            df_ann = pd.read_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_annotator{}.csv'.format(i))\n",
    "            split_dict = {x: y for x, y in zip(df['post_id'].tolist(), df['fold{}.{}'.format(repeat,k)].tolist())}\n",
    "            df_ann['fold'] = df_ann['post_id'].apply(lambda x: split_dict[x])\n",
    "            out_dict = {}\n",
    "            prec = 0\n",
    "            rec = 0\n",
    "            macroF1 = 0\n",
    "            for j, dim in enumerate(DIMS):\n",
    "                labels = df[df['fold{}.{}'.format(repeat,k)]=='TEST'][dim].tolist()\n",
    "                predictions = df_ann[df_ann['fold']=='TEST'][dim].tolist()\n",
    "                scores = precision_recall_fscore_support(labels, predictions, average='macro')\n",
    "                prec += scores[0]\n",
    "                rec += scores[1]\n",
    "                macroF1 += scores[2]\n",
    "                out_dict['eval_'+dim+'_precision'] = scores[0]\n",
    "                out_dict['eval_'+dim+'_recall'] = scores[1]\n",
    "                out_dict['eval_'+dim+'_macroF1'] = scores[2]\n",
    "            out_dict['eval_mean_precision'] = prec/len(DIMS)\n",
    "            out_dict['eval_mean_recall'] = rec/len(DIMS)\n",
    "            out_dict['eval_mean_F1'] = macroF1/len(DIMS)\n",
    "            out_dicts.append(out_dict)\n",
    "        d = {}\n",
    "        for l, _ in out_dicts[0].items():\n",
    "            d[l] = np.mean([d[l] for d in out_dicts])\n",
    "        if not os.path.isdir(data_dir+'models/human-upperbound/fold{}.{}'.format(repeat,k)):\n",
    "            os.mkdir(data_dir+'models/human-upperbound/fold{}.{}'.format(repeat,k))\n",
    "        with open(data_dir+'models/human-upperbound/fold{}.{}/test_results.json'.format(repeat,k), 'w') as f:\n",
    "            json.dump(d, f)\n",
    "        #print(out_dicts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0937dac9-58f3-4dfa-8f42-28e961fb8c1a",
   "metadata": {},
   "source": [
    "#### Majority baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b85c83-6f27-4dbb-a898-62c80a75ec66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for repeat in range(5):\n",
    "    for k in range(5):\n",
    "        split_dict = {x: y for x, y in zip(df['post_id'].tolist(), df['fold{}.{}'.format(repeat,k)].tolist())}\n",
    "        df['fold'] = df['post_id'].apply(lambda x: split_dict[x])\n",
    "        out_dict = {}\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        macroF1 = 0\n",
    "        for dim in DIMS:\n",
    "            labels = df[df['fold']=='TEST'][dim].tolist()\n",
    "            most_common = max(set(labels), key = labels.count)\n",
    "            scores = precision_recall_fscore_support(labels, [most_common for _ in range(len(labels))], average='macro')\n",
    "            prec += scores[0]\n",
    "            rec += scores[1]\n",
    "            macroF1 += scores[2]\n",
    "            out_dict['eval_'+dim+'_precision'] = scores[0]\n",
    "            out_dict['eval_'+dim+'_recall'] = scores[1]\n",
    "            out_dict['eval_'+dim+'_macroF1'] = scores[2]\n",
    "        out_dict['eval_mean_precision'] = prec/len(DIMS)\n",
    "        out_dict['eval_mean_recall'] = rec/len(DIMS)\n",
    "        out_dict['eval_mean_F1'] = macroF1/len(DIMS)\n",
    "        if not os.path.isdir(data_dir+'models/majority-baseline/fold{}.{}'.format(repeat,k)):\n",
    "            os.mkdir(data_dir+'models/majority-baseline/fold{}.{}'.format(repeat,k))\n",
    "        with open(data_dir+'models/majority-baseline/fold{}.{}/test_results.json'.format(repeat,k), 'w') as f:\n",
    "            json.dump(out_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2d1d4-efe9-4465-be1b-6077e98a244f",
   "metadata": {},
   "source": [
    "#### Random baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93514b9-ed34-4d48-8948-58d1379478ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for repeat in range(5):\n",
    "    for k in range(5):\n",
    "        split_dict = {x: y for x, y in zip(df['post_id'].tolist(), df['fold{}.{}'.format(repeat,k)].tolist())}\n",
    "        df['fold'] = df['post_id'].apply(lambda x: split_dict[x])\n",
    "        out_dict = {}\n",
    "        prec = 0\n",
    "        rec = 0\n",
    "        macroF1 = 0\n",
    "        for dim in DIMS:\n",
    "            labels = df[df['fold']=='TEST'][dim]\n",
    "            scores = precision_recall_fscore_support(labels, np.random.randint(len(list(set(labels))), size=labels.shape), average='macro')\n",
    "            prec += scores[0]\n",
    "            rec += scores[1]\n",
    "            macroF1 += scores[2]\n",
    "            out_dict['eval_'+dim+'_precision'] = scores[0]\n",
    "            out_dict['eval_'+dim+'_recall'] = scores[1]\n",
    "            out_dict['eval_'+dim+'_macroF1'] = scores[2]\n",
    "        out_dict['eval_mean_precision'] = prec/len(DIMS)\n",
    "        out_dict['eval_mean_recall'] = rec/len(DIMS)\n",
    "        out_dict['eval_mean_F1'] = macroF1/len(DIMS)\n",
    "        if not os.path.isdir(data_dir+'models/random-baseline/fold{}.{}'.format(repeat,k)):\n",
    "            os.mkdir(data_dir+'models/random-baseline/fold{}.{}'.format(repeat,k))\n",
    "        with open(data_dir+'models/random-baseline/fold{}.{}/test_results.json'.format(repeat,k), 'w') as f:\n",
    "            json.dump(out_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48759448-e4e4-4929-8081-87cd937f89cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
