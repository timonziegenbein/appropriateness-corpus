{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "725ca3f9-8db1-4b89-b1ab-81705e39abbf",
   "metadata": {},
   "source": [
    "### This file combines the annotations from our study to create the final corpus used for the automated prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13808dbb-6ff8-49b4-b393-6fc20ff9b4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import krippendorff\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ast\n",
    "import math\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from statsmodels.stats import inter_rater as irr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304ee90d-2665-4ee8-8abf-28f2ee203d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35bfb30-cf99-4ef9-8b02-7a51314ba082",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../data/'\n",
    "\n",
    "annotations_df = pd.read_csv(data_dir+'annotations/study.annotation.csv')\n",
    "annotations_df[\"result\"] = annotations_df[\"result\"].apply(ast.literal_eval)\n",
    "dataset_df = pd.read_csv(data_dir+'appropriateness-corpus/annotation_dataset_types.csv')\n",
    "\n",
    "label2type = {\"not\": 1, \"partial\": 2, \"fully\": 3}\n",
    "id_to_type = {id_: type_ for id_, type_ in zip(dataset_df['id'].tolist(),dataset_df['types'].tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61edbbcc-3979-4764-aca4-5bb393de3901",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df[\"types\"] = annotations_df[\"post_id\"].apply(lambda x: id_to_type[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcc020a-b4a2-4e17-9639-279598c74939",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Binarize the annotations\n",
    "def process_results(x, label, sub):\n",
    "    if label2type[x[\"appropriatenessQuestion\"]]!=3:\n",
    "        if not sub:\n",
    "            return 1 if x[label+\"Question\"] == 'yes' else 0\n",
    "        else:\n",
    "            if label != 'other':\n",
    "                return 1 if label in x.values() else 0 if x[label[:-1]+\"Question\"] == 'yes' else 0\n",
    "            else:\n",
    "                return 1 if label in x.keys() else 0 if x[label+\"Question\"] == 'yes' else 0\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b754bb-25ea-41bc-bd57-455b2c0e7968",
   "metadata": {},
   "outputs": [],
   "source": [
    "annotations_df[\"Appropriate (1-3 or ?)\"] = annotations_df[\"result\"].apply(lambda x: label2type[x[\"appropriatenessQuestion\"]])\n",
    "annotations_df[\"1\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'emotion', False))\n",
    "annotations_df[\"1.1\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'emotion1', True))\n",
    "annotations_df[\"1.2\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'emotion2', True))\n",
    "annotations_df[\"2\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'commitment', False))\n",
    "annotations_df[\"2.1\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'commitment1', True))\n",
    "annotations_df[\"2.2\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'commitment2', True))\n",
    "annotations_df[\"3\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'confusion', False))\n",
    "annotations_df[\"3.1\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'confusion1', True))\n",
    "annotations_df[\"3.2\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'confusion2', True))\n",
    "annotations_df[\"3.3\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'confusion3', True))\n",
    "annotations_df[\"4\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'other', False))\n",
    "annotations_df[\"4.1\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'other1', True))\n",
    "annotations_df[\"4.2\"] = annotations_df[\"result\"].apply(lambda x: process_results(x, 'other', True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4137a92-eb79-44d8-9c49-cb3024ab3e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Map annotator ids from the interface to annotators and batches\n",
    "user_dict = {\n",
    "            '6':  6,'8':  8,'7':  7, # batch0\n",
    "            '10': 6,'9':  8,'11': 7, # batch1\n",
    "            '13': 6,'12': 8,'14': 7, # batch2\n",
    "            '19': 6,'17': 8,'15': 7, # batch3\n",
    "            '21': 6,'25': 8,'18': 7, # batch4\n",
    "            '24': 6,'27': 8,'22': 7, # batch5\n",
    "            '28': 6,'30': 8,'23': 7, # batch6\n",
    "            '31': 6,'33': 8,'26': 7, # batch7\n",
    "            '35': 6,'34': 8,'29': 7, # batch8\n",
    "            '37': 6,'39': 8,'32': 7, # batch9\n",
    "            '44': 6,'40': 8,'36': 7, # batch10\n",
    "            '46': 6,'43': 8,'38': 7, # batch11\n",
    "            '48': 6,'45': 8,'41': 7, # batch12\n",
    "            '49': 6,'47': 8,'42': 7, # batch13\n",
    "            }\n",
    "\n",
    "annotations_df['user_id'] = annotations_df['user_id'].apply(lambda x: user_dict[str(x)] if str(x) in user_dict else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c077b169-5def-4657-b1ad-cb97a0632c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dfs = []\n",
    "for user in [6,7,8]:\n",
    "    tmp_df = annotations_df[annotations_df[\"user_id\"]==user].sort_values(\"post_id\")\n",
    "    tmp_df['Appropriate (binary)'] = tmp_df['Appropriate (1-3 or ?)'].apply(lambda x: 1 if x in [1,2] else 0)\n",
    "    user_dfs.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b0be68-d953-42ae-bf53-3770cca40142",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create a proper sentence from the issue\n",
    "def process_issue(x):\n",
    "    x = x.replace('-', ' ').strip().capitalize()\n",
    "    if x[-1]!= ['.','!','?',':']:\n",
    "        x = x+':'\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a70ff9-4a70-4f04-98b9-0c5bdc0c727d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create full annotator agreement corpus\n",
    "data = {\n",
    "    'post_id': user_dfs[0]['post_id'].values,\n",
    "    'source_dataset': user_dfs[0]['types'].values,\n",
    "    'issue': [process_issue(x) for x in user_dfs[0]['issue'].values],\n",
    "    'post_text': user_dfs[0]['post_text'].values,\n",
    "    'Inappropriateness': [x[0] if len(set(x))==1 else 4 for x in np.array([user_df['Appropriate (binary)'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Toxic Emotions': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Excessive Intensity': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['1.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Emotional Deception': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['1.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Commitment': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Seriousness': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['2.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Openness': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['2.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Intelligibility': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Unclear Meaning': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['3.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Relevance': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['3.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Confusing Reasoning': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['3.3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Other Reasons': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['4'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Detrimental Orthography': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['4.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Reason Unclassified': [x[0] if len(set(x)) == 1 else 4 for x in np.array([user_df['4.2'].tolist() for user_df in user_dfs]).T.tolist()]\n",
    "}\n",
    "dataset_df = pd.DataFrame(data=data)\n",
    "dataset_df.to_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_full.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfadc328-eeb9-4650-8ff5-2cd59a202682",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create majority annotator agreement corpus\n",
    "data = {\n",
    "    'post_id': user_dfs[0]['post_id'].values,\n",
    "    'source_dataset': user_dfs[0]['types'].values,\n",
    "    'issue': [process_issue(x) for x in user_dfs[0]['issue'].values],\n",
    "    'post_text': user_dfs[0]['post_text'].values,\n",
    "    'Inappropriateness': [max(set(x), key=x.count) for x in np.array([user_df['Appropriate (binary)'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Toxic Emotions': [max(set(x), key=x.count) for x in np.array([user_df['1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Excessive Intensity': [max(set(x), key=x.count) for x in np.array([user_df['1.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Emotional Deception': [max(set(x), key=x.count) for x in np.array([user_df['1.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Commitment': [max(set(x), key=x.count) for x in np.array([user_df['2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Seriousness': [max(set(x), key=x.count) == 1 for x in np.array([user_df['2.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Openness': [max(set(x), key=x.count) == 1 for x in np.array([user_df['2.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Intelligibility': [max(set(x), key=x.count) for x in np.array([user_df['3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Unclear Meaning': [max(set(x), key=x.count) for x in np.array([user_df['3.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Relevance': [max(set(x), key=x.count) for x in np.array([user_df['3.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Confusing Reasoning': [max(set(x), key=x.count) for x in np.array([user_df['3.3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Other Reasons': [max(set(x), key=x.count) for x in np.array([user_df['4'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Detrimental Orthography': [max(set(x), key=x.count) for x in np.array([user_df['4.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Reason Unclassified': [max(set(x), key=x.count) for x in np.array([user_df['4.2'].tolist() for user_df in user_dfs]).T.tolist()]\n",
    "}\n",
    "dataset_df = pd.DataFrame(data=data)\n",
    "dataset_df.to_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_majority.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3896a134-0747-4d32-abb6-47600db6884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create conservative annotator agreement corpus (used in the paper)\n",
    "data = {\n",
    "    'post_id': user_dfs[0]['post_id'].values,\n",
    "    'source_dataset': user_dfs[0]['types'].values,\n",
    "    'issue': [process_issue(x) for x in user_dfs[0]['issue'].values],\n",
    "    'post_text': user_dfs[0]['post_text'].values,\n",
    "    'Inappropriateness': [max(x) for x in np.array([user_df['Appropriate (binary)'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Toxic Emotions': [max(x) for x in np.array([user_df['1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Excessive Intensity': [max(x) for x in np.array([user_df['1.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Emotional Deception': [max(x) for x in np.array([user_df['1.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Commitment': [max(x) for x in np.array([user_df['2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Seriousness': [max(x) == 1 for x in np.array([user_df['2.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Openness': [max(x) == 1 for x in np.array([user_df['2.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Intelligibility': [max(x) for x in np.array([user_df['3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Unclear Meaning': [max(x) for x in np.array([user_df['3.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Relevance': [max(x) for x in np.array([user_df['3.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Confusing Reasoning': [max(x) for x in np.array([user_df['3.3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Other Reasons': [max(x) for x in np.array([user_df['4'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Detrimental Orthography': [max(x) for x in np.array([user_df['4.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Reason Unclassified': [max(x) for x in np.array([user_df['4.2'].tolist() for user_df in user_dfs]).T.tolist()]\n",
    "}\n",
    "dataset_df = pd.DataFrame(data=data)\n",
    "dataset_df.to_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_conservative.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609f732-6fb5-4201-9a66-33aa46c5b308",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create liberal annotator agreement corpus\n",
    "data = {\n",
    "    'post_id': user_dfs[0]['post_id'].values,\n",
    "    'source_dataset': user_dfs[0]['types'].values,\n",
    "    'issue': [process_issue(x) for x in user_dfs[0]['issue'].values],\n",
    "    'post_text': user_dfs[0]['post_text'].values,\n",
    "    'Inappropriateness': [min(x) for x in np.array([user_df['Appropriate (binary)'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Toxic Emotions': [min(x) for x in np.array([user_df['1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Excessive Intensity': [min(x) for x in np.array([user_df['1.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Emotional Deception': [min(x) for x in np.array([user_df['1.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Commitment': [min(x) for x in np.array([user_df['2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Seriousness': [min(x) == 1 for x in np.array([user_df['2.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Openness': [min(x) == 1 for x in np.array([user_df['2.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Intelligibility': [min(x) for x in np.array([user_df['3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Unclear Meaning': [min(x) for x in np.array([user_df['3.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Relevance': [min(x) for x in np.array([user_df['3.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Confusing Reasoning': [min(x) for x in np.array([user_df['3.3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Other Reasons': [min(x) for x in np.array([user_df['4'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Detrimental Orthography': [min(x) for x in np.array([user_df['4.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Reason Unclassified': [min(x) for x in np.array([user_df['4.2'].tolist() for user_df in user_dfs]).T.tolist()]\n",
    "}\n",
    "dataset_df = pd.DataFrame(data=data)\n",
    "dataset_df.to_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_liberal.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486107f0-949b-4b10-8bd5-46ef7807185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create mean annotator agreement corpus\n",
    "data = {\n",
    "    'post_id': user_dfs[0]['post_id'].values,\n",
    "    'source_dataset': user_dfs[0]['types'].values,\n",
    "    'issue': [process_issue(x) for x in user_dfs[0]['issue'].values],\n",
    "    'post_text': user_dfs[0]['post_text'].values,\n",
    "    'Inappropriateness': [np.mean(x) for x in np.array([user_df['Appropriate (binary)'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Toxic Emotions': [np.mean(x) for x in np.array([user_df['1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Excessive Intensity': [np.mean(x) for x in np.array([user_df['1.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Emotional Deception': [np.mean(x) for x in np.array([user_df['1.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Commitment': [np.mean(x) for x in np.array([user_df['2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Seriousness': [np.mean(x) for x in np.array([user_df['2.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Openness': [np.mean(x) for x in np.array([user_df['2.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Intelligibility': [np.mean(x) for x in np.array([user_df['3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Unclear Meaning': [np.mean(x) for x in np.array([user_df['3.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Missing Relevance': [np.mean(x) for x in np.array([user_df['3.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Confusing Reasoning': [np.mean(x) for x in np.array([user_df['3.3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Other Reasons': [np.mean(x) for x in np.array([user_df['4'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Detrimental Orthography': [np.mean(x) for x in np.array([user_df['4.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    'Reason Unclassified': [np.mean(x) for x in np.array([user_df['4.2'].tolist() for user_df in user_dfs]).T.tolist()]\n",
    "}\n",
    "dataset_df = pd.DataFrame(data=data)\n",
    "dataset_df.to_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_mean.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af0fcbc-8ddf-4dd7-92d2-0cd43bbc3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create predictions of each annotator in isolation (used for the human upperbound in the paper)\n",
    "for i, ann in enumerate([6,7,8]):\n",
    "    data = {\n",
    "        'post_id': user_dfs[0]['post_id'].values,\n",
    "        'source_dataset': user_dfs[0]['types'].values,\n",
    "        'issue': [process_issue(x) for x in user_dfs[0]['issue'].values],\n",
    "        'post_text': user_dfs[0]['post_text'].values,\n",
    "        'Inappropriateness': [x[i] for x in np.array([user_df['Appropriate (binary)'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Toxic Emotions': [x[i] for x in np.array([user_df['1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Excessive Intensity': [x[i] for x in np.array([user_df['1.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Emotional Deception': [x[i] for x in np.array([user_df['1.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Missing Commitment': [x[i] for x in np.array([user_df['2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Missing Seriousness': [x[i] for x in np.array([user_df['2.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Missing Openness': [x[i] for x in np.array([user_df['2.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Missing Intelligibility': [x[i] for x in np.array([user_df['3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Unclear Meaning': [x[i] for x in np.array([user_df['3.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Missing Relevance': [x[i] for x in np.array([user_df['3.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Confusing Reasoning': [x[i] for x in np.array([user_df['3.3'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Other Reasons': [x[i] for x in np.array([user_df['4'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Detrimental Orthography': [x[i] for x in np.array([user_df['4.1'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "        'Reason Unclassified': [x[i] for x in np.array([user_df['4.2'].tolist() for user_df in user_dfs]).T.tolist()],\n",
    "    }\n",
    "    dataset_df = pd.DataFrame(data=data)\n",
    "    dataset_df.to_csv(data_dir+'appropriateness-corpus/appropriateness_corpus_annotator{}.csv'.format(ann), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fbc677-96f6-4263-9671-cb999d8c99b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
